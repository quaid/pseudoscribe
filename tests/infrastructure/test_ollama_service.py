"""Tests for Ollama service"""

import pytest
from fastapi import HTTPException
from httpx import AsyncClient
from pytest_asyncio import fixture

from pseudoscribe.infrastructure.ollama_service import OllamaService, ModelInfo, GenerationResponse

@fixture(scope="function")
async def ollama_service():
    """Fixture for Ollama service instance"""
    service = OllamaService()
    yield service
    await service.client.aclose()

@fixture(scope="function")
async def check_ollama_running():
    """Fixture to check if Ollama service is running"""
    try:
        async with AsyncClient() as client:
            response = await client.get("http://localhost:11434/api/tags")
            return response.status_code == 200
    except:
        return False

@fixture(scope="function")
async def loaded_model(ollama_service, check_ollama_running):
    """Fixture to ensure model is loaded"""
    if not check_ollama_running:
        pytest.skip("Ollama service is not running")
    
    model_name = "llama2:latest"
    await ollama_service.load_model(model_name)
    return model_name

@pytest.mark.asyncio
async def test_list_models(ollama_service, check_ollama_running):
    """
    Scenario: List available Ollama models
    Given the Ollama service is running
    When listing models
    Then the list of available models is returned
    """
    # Skip test if Ollama is not running
    if not check_ollama_running:
        pytest.skip("Ollama service is not running")
    
    # Given
    service = ollama_service
    
    # When
    models = await service.list_models()
    
    # Then
    assert isinstance(models, list)
    assert len(models) > 0

@pytest.mark.asyncio
async def test_load_model(ollama_service, check_ollama_running):
    """
    Scenario: Load a model into Ollama
    Given a valid model name
    When loading the model
    Then the model is successfully loaded
    """
    # Skip test if Ollama is not running
    if not check_ollama_running:
        pytest.skip("Ollama service is not running")
    
    # Given
    service = ollama_service
    model_name = "llama2:latest"
    
    # When
    result = await service.load_model(model_name)
    
    # Then
    assert result is True

@pytest.mark.asyncio
async def test_generate(ollama_service, loaded_model):
    """
    Scenario: Generate text using a model
    Given a loaded model
    When generating text
    Then the generated text is returned
    """
    # Given
    service = ollama_service
    prompt = "Hello, how are you?"
    
    # When
    result = await service.generate(loaded_model, prompt)
    
    # Then
    assert isinstance(result, GenerationResponse)
    assert hasattr(result, "response")
    assert isinstance(result.response, str)
    assert hasattr(result, "model")
    assert result.model == loaded_model
    assert hasattr(result, "token_count")
    assert isinstance(result.token_count, int)
    assert hasattr(result, "duration")
    assert isinstance(result.duration, int)

@pytest.mark.asyncio
async def test_stream_generate(ollama_service, loaded_model):
    """
    Scenario: Stream generate text using a model
    Given a loaded model
    When streaming generation
    Then chunks of generated text are returned
    """
    # Given
    service = ollama_service
    prompt = "Hello, how are you?"
    
    # When
    async for chunk in service.stream_generate(loaded_model, prompt):
        # Then
        assert "text" in chunk
        assert isinstance(chunk["text"], str)

@pytest.mark.asyncio
async def test_model_info(ollama_service, loaded_model):
    """
    Scenario: Get model information
    Given a loaded model
    When getting model info
    Then the model's metadata is returned
    """
    # Given
    service = ollama_service
    
    # When
    info = await service.model_info(loaded_model)
    
    # Then
    assert isinstance(info, ModelInfo)
    assert info.name == loaded_model
    assert info.size > 0
    assert info.type in ["llama", "gpt", "mistral"]
